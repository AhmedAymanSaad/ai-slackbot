{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de369d279eb9478c9f78b08e84cfa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)chat.ggmlv3.q8_0.bin:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# Replace this if you want to use a different model\n",
    "model_id = \"localmodels/Llama-2-7B-Chat-ggml\"\n",
    "filenames = [ \n",
    "    \"llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=filename,\n",
    "        token=HUGGING_FACE_API_KEY\n",
    "    )\n",
    "\n",
    "    print(downloaded_model_path)\n",
    "\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 7130.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi there! My name is Sherlock Holmes."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  2283.63 ms\n",
      "llama_print_timings:      sample time =     6.76 ms /    12 runs   (    0.56 ms per token,  1774.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  3808.88 ms /    13 tokens (  292.99 ms per token,     3.41 tokens per second)\n",
      "llama_print_timings:        eval time =  3557.69 ms /    11 runs   (  323.43 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =  7410.57 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! My name is Sherlock Holmes.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "TEMPLATE = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "def get_llm_chain(model_name: str) -> LLMChain:\n",
    "    \"\"\"Get LLMChain with LlamaCpp as LLM.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the Llama model.\n",
    "\n",
    "    Returns:\n",
    "        LLMChain: LLMChain with a given model.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"question\"])\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "    llm = LlamaCpp(\n",
    "        model_path=model_name,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,\n",
    "    )\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    return llm_chain\n",
    "\n",
    "\n",
    "\n",
    "llm_chain = get_llm_chain(model_name=\"/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\")\n",
    "llm_chain.run(\"what is your name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:59:35.762005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 11:59:35.762326: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3259.66 MB (+ 2048.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model_name=\"/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_name,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_ctx=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for a given number of terms\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return 0\n",
      "    else:\n",
      "        a, b = [0, 1] * len(a)\n",
      "        return a + b + fibonacci(n-1)\n",
      "\n",
      "This code is working correctly for small values of `n`, but it becomes slow for larger values. How can I optimize it?\n",
      "\n",
      "Answer: One way to optimize this function is by using a loop that only calculates the Fibonacci sequence once and stores it in a variable, then recursively uses that value instead of recalculating it each time. This approach can greatly improve performance when `n` becomes large. Here's an example of how you could modify the function to do this:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return 0\n",
      "    else:\n",
      "        a = [0, 1] * len(a)\n",
      "        b = a[0] + a[1] + fibonacci(n-1)\n",
      "        return b\n",
      "\n",
      "In this updated version of the function, `a` and `b` are defined as for a given number of terms\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return 0\n",
      "    else:\n",
      "        a, b = [0, 1] * len(a)\n",
      "        return a + b + fibonacci(n-1)\n",
      "\n",
      "This code is working correctly for small values of `n`, but it becomes slow for larger values. How can I optimize it?\n",
      "\n",
      "Answer: One way to optimize this function is by using a loop that only calculates the Fibonacci sequence once and stores it in a variable, then recursively uses that value instead of recalculating it each time. This approach can greatly improve performance when `n` becomes large. Here's an example of how you could modify the function to do this:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return 0\n",
      "    else:\n",
      "        a = [0, 1] * len(a)\n",
      "        b = a[0] + a[1] + fibonacci(n-1)\n",
      "        return b\n",
      "\n",
      "In this updated version of the function, `a` and `b` are defined as\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   881.51 ms\n",
      "llama_print_timings:      sample time =   124.41 ms /   256 runs   (    0.49 ms per token,  2057.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1043.58 ms /    10 tokens (  104.36 ms per token,     9.58 tokens per second)\n",
      "llama_print_timings:        eval time = 31635.17 ms /   255 runs   (  124.06 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time = 33497.23 ms\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"{topic}\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"generate python code to calculate the fibonacci sequence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture03.pdf\"),\n",
    "\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:03:40.622070: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 16:03:40.660365: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 16:03:40.660758: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 16:03:41.459905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-16 16:03:42.552884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 16:03:42.553506: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import SKLearnVectorStore\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.embeddings import TensorflowHubEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# embeddings = LlamaCppEmbeddings(model_path=model_name, n_ctx=4098)\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings = TensorflowHubEmbeddings()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "doc_result = embeddings.embed_documents([text, \"This is not a test document.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(query_result)\n",
    "# print(doc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vector_store.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"MachineLearning-Lecture02  \\nInstructor (Andrew Ng) :All right, good morning, welcom e back. So before we jump \\ninto today's material, I just have one admini strative announcement, which is graders. So I \\nguess sometime next week, we'll hand out the fi rst homework assignment for this class.  \\nIs this loud enough, by the way? Can people in  the back hear me? No. Can you please \\nturn up the mic a bit louder? Is this bette r? Is this okay? This is okay? Great.  \\nSo sometime next week, we'll hand out the firs t problem sets and it'll be two weeks after \\nthat, and the way we grade homework problems in this class is by some combination of \\nTAs and graders, where graders are usually me mbers â€“ students currently in the class.  \\nSo in maybe about a week or so, I'll email the class to solicit applica tions for those of you \\nthat might be interested in becoming graders fo r this class, and ther e's usually sort of a \\nfun thing to do. So four times this quarter, th e TAs, and the graders, and I will spend one \\nevening staying up late and grad ing all the homework problems.  \\nFor those of you who that have never taught a cla ss before, or sort of been a grader, it's an \\ninteresting way for you to see, you know, what  the other half of the teaching experience \\nis. So the students that grade for the first time sort of get to learn about what it is that \\nreally makes a difference between a good so lution and amazing solution. And to give\" metadata={'source': 'docs/MachineLearning-Lecture02.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    # chain_type=\"refine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Major topics for this class include but are not limited to: 1) The history of AI and ML, 2) Supervised and unsupervised learning, 3) Neural networks, and 4) Specialized applications such as natural language processing and computer vision."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   773.46 ms\n",
      "llama_print_timings:      sample time =    28.55 ms /    58 runs   (    0.49 ms per token,  2031.31 tokens per second)\n",
      "llama_print_timings: prompt eval time = 131448.73 ms /  1386 tokens (   94.84 ms per token,    10.54 tokens per second)\n",
      "llama_print_timings:        eval time =  8670.70 ms /    57 runs   (  152.12 ms per token,     6.57 tokens per second)\n",
      "llama_print_timings:       total time = 140604.27 ms\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are major topics for this class?',\n",
       " 'result': ' Major topics for this class include but are not limited to: 1) The history of AI and ML, 2) Supervised and unsupervised learning, 3) Neural networks, and 4) Specialized applications such as natural language processing and computer vision.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* map reduce retrival \n",
    "    * answer: The major topics for this class are machine learning and artificial intelligence.\n",
    "    * time: 9m 14s\n",
    "* stuff retrival \n",
    "    * answer: The following subjects will be covered in this course:  1. Linear Regression: This topic will cover basic and advanced concepts of linear regression, including modeling assumptions, regularization, and prediction-making use of the technique to model and forecast housing prices and other relevant features.  2. Gradient Descent: This subject will investigate the gradient descent method for optimizing parameters in machine learning models. Topics covered will include understanding the objective function, choosing the right update rule, and managing the learning rate.  3. Normals Equations: In this topic, we'll explore normal equations, which are used to model relationships between variables. Normal equations can be used for linear regression, nonlinear regression, and other machine learning techniques.  4. Linear Algebra: This subject will introduce fundamental ideas in linear algebra, including matrices, vectors, and matrix multiplication. We will also go over some real-world issues where linear algebra is utilized, such as image and signal processing.  5. Machine Learning: This topic will investigate the fundamentals of machine learning, including supervised and unsupervised learning methods like clustering and classification. We will examine how to use these techniques to make accurate predictions about a target variable in a given dataset.\n",
    "    * time: 6m 20s\n",
    "* refine\n",
    "    * answer: \n",
    "        * Refined Answer:\n",
    "        The major topics for this class are likely to be programming intensive, but not very program-ming extensive, as the instructor will only use MATLAB or Octave for some tasks. These topics may include probability and statistics, linear algebra, and basics of machine learning. The class will also involve discussions on prerequisites and going over material as a refresher course under the prerequisite class. Additionally, the instructor will address the fact that videos or pictures of students in the classroom will not be posted online, but microphones may pick up voices, so students should avoid yelling out swear words during class.\n",
    "\n",
    "        * Original Answer:\n",
    "        The major topics for this class are likely to be programming intensive, although we will do some programming, mostly in either MATLAB or Octave. We will also cover probability and statistics, linear algebra, and basics of machine learning. The class may involve discussions on prerequisites and going over material as a refresher course under the prerequisite class.\n",
    "    * time: 6m 2s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement: Sent the splits instead of the full documents to the vector store gave an acceptable answer using the stuff method in 2m 20s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
