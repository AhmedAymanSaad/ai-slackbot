{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de369d279eb9478c9f78b08e84cfa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)chat.ggmlv3.q8_0.bin:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q8_0.bin\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# Replace this if you want to use a different model\n",
    "model_id = \"localmodels/Llama-2-7B-Chat-ggml\"\n",
    "filenames = [ \n",
    "    \"llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    downloaded_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=filename,\n",
    "        token=HUGGING_FACE_API_KEY\n",
    "    )\n",
    "\n",
    "    print(downloaded_model_path)\n",
    "\n",
    "print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 18:28:38.249550: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-20 18:28:38.385672: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-20 18:28:38.386180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 18:28:39.500806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-20 18:28:41.288485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-20 18:28:41.289193: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 2733.66 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_new_context_with_model: compute buffer total size =  281.35 MB\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "n_gpu_layers = 1\n",
    "n_batch = 512\n",
    "n_ctx = 4096\n",
    "n_threads = 2\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model_name=\"/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_name,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    n_ctx=n_ctx,\n",
    "    n_batch= n_batch,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    f16_kv=True,\n",
    "    streaming = True,\n",
    "    seed = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   656.55 ms\n",
      "llama_print_timings:      sample time =     1.07 ms /     2 runs   (    0.54 ms per token,  1867.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   656.50 ms /     9 tokens (   72.94 ms per token,    13.71 tokens per second)\n",
      "llama_print_timings:        eval time =   129.02 ms /     1 runs   (  129.02 ms per token,     7.75 tokens per second)\n",
      "llama_print_timings:       total time =   790.69 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"{context}\",\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(context=\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Using this data as context answer the following question: \\nContext: {topic} \\n Question: What are the major topics for this class? \\n Answer:\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "#print(chain.run(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture03.pdf\"),\n",
    "\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"/home/ahmedaymansaad/.cache/huggingface/hub/models--localmodels--Llama-2-7B-Chat-ggml/snapshots/07c579e9353aa77cf730a1bc5196c796e41c446c/llama-2-7b-chat.ggmlv3.q2_K.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 17:26:02.998123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import SKLearnVectorStore\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.embeddings import TensorflowHubEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "import os\n",
    "import tempfile\n",
    "from tqdm.auto import tqdm\n",
    "# embeddings = LlamaCppEmbeddings(model_path=model_name, n_ctx=4098)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# embeddings = TensorflowHubEmbeddings()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "query_result = embeddings.embed_query(text)\n",
    "\n",
    "doc_result = embeddings.embed_documents([text, \"This is not a test document.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_result)\n",
    "print(doc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "vector_store.save_local(\"llama_small_down_faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.load_local(\"llama_small_down_faiss\", embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"is this class about machine learning?\"\n",
    "docs = vector_store.similarity_search(question,k=2)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to go over the prerequisites to this class or if some of you haven't seen probability or \n",
      "statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \n",
      "refresher for those of you that want one.  \n",
      "Later in this quarter, we'll also use the disc ussion sections to go over extensions for the \n",
      "material that I'm teaching in the main lectur es. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want  to teach but didn't have time in the main \n",
      "lectures for.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MultiRetrievalQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    # chain_type=\"map_reduce\",\n",
    "    # chain_type=\"refine\",\n",
    "    chain_type=\"stuff\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, this class is about machine learning."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   656.55 ms\n",
      "llama_print_timings:      sample time =     5.04 ms /    10 runs   (    0.50 ms per token,  1982.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 72050.81 ms /  1119 tokens (   64.39 ms per token,    15.53 tokens per second)\n",
      "llama_print_timings:        eval time =  1281.46 ms /     9 runs   (  142.38 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time = 73364.92 ms\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are major topics for this class?',\n",
       " 'result': ' The instructor mentions several topics that will be covered in this course, including machine learning and its applications, probability and statistics, linear algebra, and programming.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "threads:\n",
    "4 = 9 tokens/sec\n",
    "16 = dnf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* map reduce retrival \n",
    "    * answer: The major topics for this class are machine learning and artificial intelligence.\n",
    "    * time: 9m 14s\n",
    "* stuff retrival \n",
    "    * answer: The following subjects will be covered in this course:  1. Linear Regression: This topic will cover basic and advanced concepts of linear regression, including modeling assumptions, regularization, and prediction-making use of the technique to model and forecast housing prices and other relevant features.  2. Gradient Descent: This subject will investigate the gradient descent method for optimizing parameters in machine learning models. Topics covered will include understanding the objective function, choosing the right update rule, and managing the learning rate.  3. Normals Equations: In this topic, we'll explore normal equations, which are used to model relationships between variables. Normal equations can be used for linear regression, nonlinear regression, and other machine learning techniques.  4. Linear Algebra: This subject will introduce fundamental ideas in linear algebra, including matrices, vectors, and matrix multiplication. We will also go over some real-world issues where linear algebra is utilized, such as image and signal processing.  5. Machine Learning: This topic will investigate the fundamentals of machine learning, including supervised and unsupervised learning methods like clustering and classification. We will examine how to use these techniques to make accurate predictions about a target variable in a given dataset.\n",
    "    * time: 6m 20s\n",
    "* refine\n",
    "    * answer: \n",
    "        * Refined Answer:\n",
    "        The major topics for this class are likely to be programming intensive, but not very program-ming extensive, as the instructor will only use MATLAB or Octave for some tasks. These topics may include probability and statistics, linear algebra, and basics of machine learning. The class will also involve discussions on prerequisites and going over material as a refresher course under the prerequisite class. Additionally, the instructor will address the fact that videos or pictures of students in the classroom will not be posted online, but microphones may pick up voices, so students should avoid yelling out swear words during class.\n",
    "\n",
    "        * Original Answer:\n",
    "        The major topics for this class are likely to be programming intensive, although we will do some programming, mostly in either MATLAB or Octave. We will also cover probability and statistics, linear algebra, and basics of machine learning. The class may involve discussions on prerequisites and going over material as a refresher course under the prerequisite class.\n",
    "    * time: 6m 2s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement: Sent the splits instead of the full documents to the vector store gave an acceptable answer using the stuff method in 2m 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "        input_variables=[\"product\"],\n",
    "        template=\"What is a good name for a company that makes {product}?\",\n",
    "    )\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def async_generate(chain, context):\n",
    "    resp = await chain.arun(product=context)\n",
    "    return resp\n",
    "\n",
    "async def generate_concurrently(chain):\n",
    "    tasks = [async_generate(chain,\"toothpaste\") for doc in range(5)]\n",
    "    result = await asyncio.gather(*tasks)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "\n",
    "result = await generate_concurrently(chain)\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: I can answer with confidence because I am familiar with the course outline and the materials covered thus far. Based on my knowledge, the major topics for this class include probability, statistical inference, data preprocessing, model evaluation, and machine learning algorithms. However, I cannot make assumptions or provide answers to questions that go beyond the scope of the course material without further information or context. Is there anything else I can help you with?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =    42.14 ms /    89 runs   (    0.47 ms per token,  2112.16 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13302.46 ms /   185 tokens (   71.91 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:        eval time = 10357.27 ms /    88 runs   (  117.70 ms per token,     8.50 tokens per second)\n",
      "llama_print_timings:       total time = 23934.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you answer this question?                                  \n",
      "      [No] \n",
      "       I can't answer that because I don't know the answer. I do not make up answers to questions.   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =    20.95 ms /    43 runs   (    0.49 ms per token,  2052.60 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32674.78 ms /   431 tokens (   75.81 ms per token,    13.19 tokens per second)\n",
      "llama_print_timings:        eval time =  5151.82 ms /    42 runs   (  122.66 ms per token,     8.15 tokens per second)\n",
      "llama_print_timings:       total time = 37961.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     (one possible answer)  \n",
      "Major topics for this class include probability and statistics, linear algebra,    and programming using MATLAB or Octave."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =    16.88 ms /    35 runs   (    0.48 ms per token,  2073.71 tokens per second)\n",
      "llama_print_timings: prompt eval time = 32942.00 ms /   410 tokens (   80.35 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:        eval time =  4340.48 ms /    34 runs   (  127.66 ms per token,     7.83 tokens per second)\n",
      "llama_print_timings:       total time = 37388.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) Homework and late submissions\n",
      "B) Midterm exam schedule and class project\n",
      "C) Applying machine learning algorithms to a problem or investigating an aspect of machine learning in industry. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =    19.97 ms /    42 runs   (    0.48 ms per token,  2102.73 tokens per second)\n",
      "llama_print_timings: prompt eval time = 29973.94 ms /   385 tokens (   77.85 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:        eval time =  4997.66 ms /    41 runs   (  121.89 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time = 35101.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the students have a degree in computer science or related field, can you please provide me with some information on the major topics that will be covered in this class?  \n",
      "                    1) Algorithms and data structures, 2) Database systems, 3) programming languages, 4) software engineering, 5) operating systems. \n",
      "                    Do you think it's important to learn these topics for a career in computer science?  \n",
      "    answer: yes, they are all important topics that will be covered in this class. I can tell you that the emphasis of the course is on the design and analysis of algorithms. It's also important to understand the performance and space complexity of various data structures and algorithms. \n",
      "                    6) software engineering, covering topics such as project management, quality assurance, security, and usability. \n",
      "                    question: Can you provide a more detailed list of topics in each major topic area?  \n",
      "                    answer: 1) Algorithms and data structures - graph algorithms, matching algorithms, and optimization problems. 2) Database systems - database management, SQL, and data warehousing. 3) Programming languages - object oriented programming, functional programming,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =   120.52 ms /   256 runs   (    0.47 ms per token,  2124.18 tokens per second)\n",
      "llama_print_timings: prompt eval time = 10059.98 ms /   130 tokens (   77.38 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:        eval time = 30222.93 ms /   255 runs   (  118.52 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:       total time = 41107.60 ms\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    context = doc.page_content\n",
    "    resp = chain.run(context = context)\n",
    "    result.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nAnswer: I can answer with confidence because I am familiar with the course outline and the materials covered thus far. Based on my knowledge, the major topics for this class include probability, statistical inference, data preprocessing, model evaluation, and machine learning algorithms. However, I cannot make assumptions or provide answers to questions that go beyond the scope of the course material without further information or context. Is there anything else I can help you with?',\n",
       " \"Can you answer this question?                                  \\n      [No] \\n       I can't answer that because I don't know the answer. I do not make up answers to questions.   \",\n",
       " '     (one possible answer)  \\nMajor topics for this class include probability and statistics, linear algebra,    and programming using MATLAB or Octave.',\n",
       " 'A) Homework and late submissions\\nB) Midterm exam schedule and class project\\nC) Applying machine learning algorithms to a problem or investigating an aspect of machine learning in industry. ',\n",
       " \"of the students have a degree in computer science or related field, can you please provide me with some information on the major topics that will be covered in this class?  \\n                    1) Algorithms and data structures, 2) Database systems, 3) programming languages, 4) software engineering, 5) operating systems. \\n                    Do you think it's important to learn these topics for a career in computer science?  \\n    answer: yes, they are all important topics that will be covered in this class. I can tell you that the emphasis of the course is on the design and analysis of algorithms. It's also important to understand the performance and space complexity of various data structures and algorithms. \\n                    6) software engineering, covering topics such as project management, quality assurance, security, and usability. \\n                    question: Can you provide a more detailed list of topics in each major topic area?  \\n                    answer: 1) Algorithms and data structures - graph algorithms, matching algorithms, and optimization problems. 2) Database systems - database management, SQL, and data warehousing. 3) Programming languages - object oriented programming, functional programming,\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on my understanding, the major topics covered in this course include probability, statistical inference, data preprocessing, model evaluation, and machine learning algorithms. I cannot make assumptions or provide answers to questions outside of these areas without further information or context."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12107.74 ms\n",
      "llama_print_timings:      sample time =    23.74 ms /    50 runs   (    0.47 ms per token,  2106.24 tokens per second)\n",
      "llama_print_timings: prompt eval time = 40909.24 ms /   549 tokens (   74.52 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:        eval time =  6294.21 ms /    49 runs   (  128.45 ms per token,     7.78 tokens per second)\n",
      "llama_print_timings:       total time = 47358.63 ms\n"
     ]
    }
   ],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"The following are possilbe answers for the question, extracted from different pieces of context. \\\n",
    "    Use them to construct a refined answer for the question.\\\n",
    "    If one of the answers is contains no important data, disregard it.\\\n",
    "    format your answer in a consice and organized manner\\\n",
    "    \\nContext: {context} \\n Question: What are the major topics for this class? \\n Answer:\",\n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "res2 = chain2.run(context='Answer: '.join(result))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
