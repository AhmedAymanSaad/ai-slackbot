{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuNNsCc67IvG",
        "outputId": "023b93b4-6fac-4aff-c8f4-4fef8aa0978b"
      },
      "outputs": [],
      "source": [
        "!pip install qdrant-client langchain  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Qdrant      \n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "import qdrant_client\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a qdrant client \n",
        "\n",
        "qdrant_client = QdrantClient(host='localhost', port=6333)\n",
        "\n",
        "# os.environ['QDRANT_HOST'] = \"http://localhost:6333\"\n",
        "\n",
        "# client= qdrant_client.QdrantClient(host='localhost', port=6333)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create collection\n",
        "\n",
        "os.environ['QDRANT_COLLECTION1_NAME'] = \"github-collection\"\n",
        "os.environ['QDRANT_COLLECTION2_NAME'] = \"slack-collection\"\n",
        "\n",
        "#the size is accoridng to the model\n",
        "#1536 openAI embeddings, 768 for insturctor-xl\n",
        "#size of the vectors returned by the embeddings model\n",
        "\n",
        "vectors_config= VectorParams(size=384, distance=Distance.COSINE)\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name= os.getenv(\"QDRANT_COLLECTION1_NAME\"),\n",
        "    vectors_config= vectors_config,\n",
        ")\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name= os.getenv(\"QDRANT_COLLECTION2_NAME\"),\n",
        "    vectors_config= vectors_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create vector store\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"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\"\n",
        "\n",
        "embeddings_model_name=\"all-MiniLM-L6-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "github_vector_store= Qdrant(\n",
        "    client= qdrant_client,\n",
        "    collection_name= os.getenv(\"QDRANT_COLLECTION1_NAME\"),\n",
        "    embeddings=embeddings,\n",
        "    )\n",
        "\n",
        "slack_vector_store= Qdrant(\n",
        "    client= qdrant_client,\n",
        "    collection_name= os.getenv(\"QDRANT_COLLECTION2_NAME\"),\n",
        "    embeddings=embeddings,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --upgrade unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x8d in position 4328: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# loader= CSVLoader(file_path='../initial_docs/discussions_dataset.csv')\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# docs= loader.load()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loader \u001b[39m=\u001b[39m CSVLoader(\n\u001b[0;32m     12\u001b[0m     file_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../initial_docs/discussions_dataset.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     csv_args\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     },\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m     22\u001b[0m \u001b[39m# loader = UnstructuredCSVLoader(file_path=\"../initial_docs/discussions_dataset.csv\", mode=\"elements\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# docs = loader.load()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m text_splitter \u001b[39m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, chunk_overlap\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Mahinour Elsarky\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\langchain\\document_loaders\\csv_loader.py:51\u001b[0m, in \u001b[0;36mCSVLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path, newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[0;32m     50\u001b[0m     csv_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mDictReader(csvfile, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsv_args)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(csv_reader):\n\u001b[0;32m     52\u001b[0m         content \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mv\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m row\u001b[39m.\u001b[39mitems())\n\u001b[0;32m     53\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Mahinour Elsarky\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\csv.py:111\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    109\u001b[0m     \u001b[39m# Used only for its side effect.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfieldnames\n\u001b[1;32m--> 111\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader)\n\u001b[0;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_num \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mline_num\n\u001b[0;32m    114\u001b[0m \u001b[39m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m# values\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mahinour Elsarky\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 4328: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "# add csv files to vector stores   - CSV LOADER for github\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders.csv_loader import CSVLoader, UnstructuredCSVLoader\n",
        "\n",
        "\n",
        "\n",
        "# loader= CSVLoader(file_path='../initial_docs/discussions_dataset.csv')\n",
        "# docs= loader.load()\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path='../initial_docs/discussions_dataset.csv',\n",
        "    csv_args={\n",
        "        \"delimiter\": \",\",\n",
        "        \"quotechar\": '\"',\n",
        "        \"fieldnames\": [\"Discussion_ID\", \"Comment_ID\", \"Author\", \"Category\", \"Markup_Body\", \"Body\", \"Created At\", \"Last Edited At\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# loader = UnstructuredCSVLoader(file_path=\"../initial_docs/discussions_dataset.csv\", mode=\"elements\")\n",
        "# docs = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "github_docs=text_splitter.split_documents(docs)\n",
        "\n",
        "#print(docs[0].metadata[\"text_as_html\"])\n",
        "print(github_docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jq\n",
            "  Using cached jq-1.5.0.tar.gz (2.7 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: jq\n",
            "  Building wheel for jq (pyproject.toml): started\n",
            "  Building wheel for jq (pyproject.toml): finished with status 'error'\n",
            "Failed to build jq\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for jq (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [5 lines of output]\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      Executing: ./configure CFLAGS=-fPIC --prefix=C:\\Users\\Mahinour Elsarky\\AppData\\Local\\Temp\\pip-install-bttb3rau\\jq_7b1c9f772e6447e79f51a0f4d7d488fc\\_deps\\build\\onig-install-6.9.4\n",
            "      error: [WinError 2] The system cannot find the file specified\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for jq\n",
            "ERROR: Could not build wheels for jq, which is required to install pyproject.toml-based projects\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        " !pip install jq\n",
        "\n",
        "#I have an error here as i think there is a problem with jq on windows --Mahy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add json files to vector stores   - JSON LOADER for slack\n",
        "\n",
        "###########################\n",
        "# You will need to run the above cell (pip install jq)\n",
        "#For the one who will run this on his linux computer you can refer to this documentation: https://python.langchain.com/docs/modules/data_connection/document_loaders/json\n",
        "#I was still trying to test, so i don't know if the code I wrote works as excpeted \n",
        "#Also we want to do a for loop to add all attendance json files together \n",
        "#And we need to do the same for general json files\n",
        "#For the users.json its just one file\n",
        "# The files are in the directory initial_docs\n",
        "#Note that the users.json i think its json_lines so we need to set this to true\n",
        "# Also we can use the metafunc in the documentation, please refer to the docs \n",
        "# ---- MAHY wrote this 31/8/2023\n",
        "###############################\n",
        "\n",
        "from langchain.document_loaders import JSONLoader \n",
        "#from json_loader_copy import JSONLoader    --This was a trial from me to try use it without jq but failed, i found the issue here: https://github.com/langchain-ai/langchain/issues/4396\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "\n",
        "# step 1 trial:\n",
        "\n",
        "attendance_file_path='../initial_docs/attendance/2023-07-19.json'\n",
        "#attendance_data = json.loads(Path(attendance_file_path).read_text())\n",
        "#pprint(attendance_data)\n",
        "\n",
        "attendance_loader = JSONLoader(\n",
        "    file_path= attendance_file_path,\n",
        "    jq_schema='.',\n",
        "    content_key='text'\n",
        "   )\n",
        "\n",
        "#attendance_data = attendance_loader.load()\n",
        "\n",
        "#pprint(attendance_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2 - attendance channel - for loop on all the json files and add it to [attendance_data]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 3 - users.json\n",
        "\n",
        "\n",
        "\n",
        "# step 4  - general channel:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 5  - gathering all of them in one [slack_docs[] variable which is later used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text splitter of all docs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "#we can change the chunk size according to the model max tokens  -- i am not sure\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "github_docs_splitted = text_splitter.split_documents(github_docs)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "slack_docs_splitted = text_splitter.split_documents(slack_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connecting to qdrant in langchain & loading the docs\n",
        "\n",
        "url = 'localhost'\n",
        "qdrant = Qdrant.from_documents(\n",
        "    github_docs_splitted,\n",
        "    embeddings,\n",
        "    url=url,\n",
        "    prefer_grpc=True,\n",
        "    collection_name=os.getenv(\"QDRANT_COLLECTION1_NAME\"),\n",
        ")\n",
        "\n",
        "qdrant = Qdrant.from_documents(\n",
        "    slack_docs_splitted,\n",
        "    embeddings,\n",
        "    url=url,\n",
        "    prefer_grpc=True,\n",
        "    collection_name=os.getenv(\"QDRANT_COLLECTION2_NAME\"),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
